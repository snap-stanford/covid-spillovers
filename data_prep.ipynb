{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting numpy cores to 1\n",
      "Running code on rambo.stanford.edu; at Stanford=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch1/safegraph_homes/covid_mobility_venv/covid_mobility_venv/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopy.distance as gd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "import helper_methods_for_aggregate_data_analysis as helper\n",
    "import covid_constants_and_util as cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep `visits`, `index`, and `cbg_device_counts` for entire US (run this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to keep from SafeGraph Weekly Patterns\n",
    "cols_to_keep_weekly = [\n",
    "    'safegraph_place_id',\n",
    "    'visitor_home_cbgs',\n",
    "    'date_range_start',\n",
    "    'poi_cbg',\n",
    "]\n",
    "\n",
    "# columns to keep from SafeGraph Places\n",
    "cols_to_keep_places = [\n",
    "    'safegraph_place_id',\n",
    "    'city',\n",
    "    'region',\n",
    "    'top_category',\n",
    "    'sub_category',\n",
    "]\n",
    "\n",
    "path_1 = os.path.join(cu.CURRENT_DATA_DIR, 'weekly_pre_20200615/main-file/')\n",
    "path_2 = os.path.join(cu.CURRENT_DATA_DIR, 'weekly_post_20200615/patterns/')\n",
    "path_3 = os.path.join(cu.CURRENT_DATA_DIR, 'weekly_post_20201130/patterns/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part1.csv.gz\n",
      "[########################################] | 100% Completed |  9.6s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part2.csv.gz\n",
      "[########################################] | 100% Completed |  9.7s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part3.csv.gz\n",
      "[########################################] | 100% Completed |  8.6s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part4.csv.gz\n",
      "[########################################] | 100% Completed | 10.8s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part5.csv.gz\n",
      "[########################################] | 100% Completed | 11.8s\n",
      "Loading core places info for 5333501 POIs\n",
      "220333 rows of 2018 1-year ACS data read\n",
      "217739 rows of 2017 5-year ACS data read\n"
     ]
    }
   ],
   "source": [
    "places = helper.load_core_places_data(cols_to_keep_places)\n",
    "acs_d = helper.load_and_reconcile_multiple_acs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31\n",
      "2019-01-07\n",
      "2019-01-14\n",
      "2019-01-21\n",
      "2019-01-28\n",
      "2019-02-04\n",
      "2019-02-11\n",
      "2019-02-18\n",
      "2019-02-25\n",
      "2019-03-04\n",
      "2019-03-11\n",
      "2019-03-18\n",
      "2019-03-25\n",
      "2019-04-01\n",
      "2019-04-08\n",
      "2019-04-15\n",
      "2019-04-22\n",
      "2019-04-29\n",
      "2019-05-06\n",
      "2019-05-13\n",
      "2019-05-20\n",
      "2019-05-27\n",
      "2019-06-03\n",
      "2019-06-10\n",
      "2019-06-17\n",
      "2019-06-24\n",
      "2019-07-01\n",
      "2019-07-08\n",
      "2019-07-15\n",
      "2019-07-22\n",
      "2019-07-29\n",
      "2019-08-05\n",
      "2019-08-12\n",
      "2019-08-19\n",
      "2019-08-26\n",
      "2019-09-02\n",
      "2019-09-09\n",
      "2019-09-16\n",
      "2019-09-23\n",
      "2019-09-30\n",
      "2019-10-07\n",
      "2019-10-14\n",
      "2019-10-21\n",
      "2019-10-28\n",
      "2019-11-04\n",
      "2019-11-11\n",
      "2019-11-18\n",
      "2019-11-25\n",
      "2019-12-02\n",
      "2019-12-09\n",
      "2019-12-16\n",
      "2019-12-23\n",
      "2019-12-30\n",
      "2020-01-06\n",
      "2020-01-13\n",
      "2020-01-20\n",
      "2020-01-27\n",
      "2020-02-03\n",
      "2020-02-10\n",
      "2020-02-17\n",
      "2020-02-24\n",
      "2020-03-02\n",
      "2020-03-09\n",
      "2020-03-16\n",
      "2020-03-23\n",
      "2020-03-30\n",
      "2020-04-06\n",
      "2020-04-13\n",
      "2020-04-20\n",
      "2020-04-27\n",
      "2020-05-04\n",
      "2020-05-11\n",
      "2020-05-18\n",
      "2020-05-25\n",
      "2020-06-01\n",
      "2020-06-08\n",
      "2020-06-24\n",
      "2020-07-01\n",
      "2020-07-08\n",
      "2020-07-15\n",
      "2020-07-22\n",
      "2020-07-29\n",
      "2020-08-05\n",
      "2020-08-12\n",
      "2020-08-19\n",
      "2020-08-26\n",
      "2020-09-02\n",
      "2020-09-09\n",
      "2020-09-16\n",
      "2020-09-23\n",
      "2020-09-30\n",
      "2020-10-07\n",
      "2020-10-14\n",
      "2020-10-21\n",
      "2020-10-28\n",
      "2020-11-04\n",
      "2020-11-11\n",
      "2020-11-18\n",
      "2020-11-25\n",
      "2020-12-02\n",
      "2020-12-09\n",
      "2020-12-16\n",
      "2020-12-23\n",
      "2020-12-30\n",
      "2021-01-06\n",
      "2021-01-13\n",
      "2021-01-20\n",
      "2021-01-27\n",
      "2021-02-03\n",
      "2021-02-10\n",
      "2021-02-17\n",
      "2021-02-24\n",
      "2021-03-03\n",
      "2021-03-11\n",
      "2021-03-17\n",
      "2021-03-24\n",
      "2021-03-31\n",
      "2021-04-07\n",
      "2021-04-14\n",
      "2021-04-21\n",
      "2021-04-28\n",
      "2021-05-05\n",
      "2021-05-13\n",
      "2021-05-19\n",
      "2021-05-26\n",
      "2021-06-02\n",
      "2021-06-09\n",
      "2021-06-16\n",
      "2021-06-23\n",
      "2021-06-30\n",
      "2021-07-07\n",
      "2021-07-14\n",
      "2021-07-21\n",
      "2021-07-28\n",
      "2021-08-04\n",
      "2021-08-11\n"
     ]
    }
   ],
   "source": [
    "# Construct DF containing data for each week from Weekly Patterns\n",
    "correct_column_dates = {}\n",
    "weekly_dfs = []\n",
    "poi_cbg_sers = []\n",
    "poi_dwell_sers = []\n",
    "for path in [path_1, path_2, path_3]: # Loop over Three data collection \"periods\"\n",
    "    path_is_pre = 'weekly_pre' in path\n",
    "    week_paths = sorted(glob.glob(os.path.join(path, '*')) if path_is_pre else glob.glob(os.path.join(path, '*/*/*/*')))\n",
    "    for week_path in week_paths: # Loop over each week in the data collection period\n",
    "        week_str = week_path.split('/')[-1][:10] if path_is_pre else '-'.join(week_path.split('/')[-4:-1])\n",
    "        if (week_str == '2020-12-09') and (path == path_3): # Duplicates in path_2 and path_3\n",
    "            continue\n",
    "        print(week_str)\n",
    "        weekly_df = helper.load_weekly_patterns_v2_data(week_str, cols_to_keep_weekly, expand_hourly_visits=False)        \n",
    "        correct_column_dates[week_str + '.visitor_home_cbgs'] = weekly_df.iloc[0].date_range_start.split('T')[0] + '.visitor_home_cbgs'\n",
    "        poi_cbg_sers.append(weekly_df.poi_cbg)  # the CBG of the POI\n",
    "        poi_dwell_sers.append(weekly_df.median_dwell.rename(week_str+'.median_dwell'))\n",
    "        weekly_df = weekly_df.drop(columns=['date_range_start', 'poi_cbg', 'median_dwell']).rename({'visitor_home_cbgs': week_str+'.visitor_home_cbgs'}, axis=1)\n",
    "        weekly_dfs.append(weekly_df)\n",
    "weekly_df = pd.concat(weekly_dfs, axis=1, sort=False).fillna('{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting latest CBG registered for POI that is not NaN. POI CBGs are found in the weekly dataset.\n",
    "\n",
    "def extract_cbg(ser):\n",
    "    cbg = ser.values\n",
    "    latest_idx = np.where((~np.isnan(cbg)))[0][-1]\n",
    "    return cbg[latest_idx]\n",
    "\n",
    "poi_cbgs = pd.concat(poi_cbg_sers, axis=1, sort=False)\n",
    "poi_cbgs = poi_cbgs[~poi_cbgs.isna().all(axis=1)]  # keep POIs that are not all NaN for poi_cbg\n",
    "poi_cbgs = poi_cbgs.apply(extract_cbg, axis=1).rename('poi_cbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating index variable\n",
    "visit_cols = [col for col in correct_column_dates.values() if col.endswith('.visitor_home_cbgs')]\n",
    "weeks = [visit_col.strip('.visitor_home_cbgs') for visit_col in visit_cols]\n",
    "pois = places.index[places.index.isin(weekly_df.index)].tolist()\n",
    "cbgs = acs_d.census_block_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/dvrabac/miniconda3/envs/mobility/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Gather Label and Feature Data in one DF\n",
    "places = places.join(poi_cbgs, how='inner').loc[pois, :]\n",
    "weekly_df = weekly_df.rename(correct_column_dates, axis=1).loc[pois, :]\n",
    "poi_dwell = pd.concat(poi_dwell_sers, axis=1, sort=False).loc[pois,:]\n",
    "poi_dwell.index.name = 'safegraph_place_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visitor_home_cbgs from SafeGraph are type string, need to transform to dict\n",
    "for visit_col in visit_cols:\n",
    "    if type(weekly_df.iloc[0][visit_col]) != Counter:\n",
    "        weekly_df[visit_col] = weekly_df[visit_col].fillna('{}').map(lambda d: Counter(helper.cast_keys_to_ints(json.loads(d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing CBG -> POI Visits Matrices\n",
    "cbg2idx = {c:i for i, c in enumerate(cbgs)}\n",
    "visits = []\n",
    "total = len(cbgs) * len(pois)\n",
    "for w, week in enumerate(weeks):\n",
    "    poi_idx = []\n",
    "    cbg_idx = []\n",
    "    visit_data = []\n",
    "    for p, poi_id in enumerate(pois):\n",
    "        visits_wp = weekly_df.loc[poi_id, f\"{week}.visitor_home_cbgs\"]\n",
    "        for src_cbg, num_visits in visits_wp.items():\n",
    "            if src_cbg in cbg2idx:\n",
    "                poi_idx.append(p)\n",
    "                cbg_idx.append(cbg2idx[src_cbg])\n",
    "                visit_data.append(num_visits)\n",
    "    visits.append(sparse.csr_matrix((visit_data, (cbg_idx, poi_idx)), shape=(len(cbgs), len(pois))))\n",
    "    print('Found %d visits (%.3f%% of matrix)' % (len(cbg_idx), len(cbg_idx) * 100 / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all POIs with poi_cbg info\n",
    "mask = ~places.poi_cbg.isna().to_numpy()\n",
    "poi_cbgs = places.loc[mask, 'poi_cbg'].astype(int).tolist()\n",
    "poi_dwell = poi_dwell.loc[mask, :]\n",
    "visits = [visit[mask, :] for visit in visits]\n",
    "pois = [poi for poi, m in zip(pois, mask) if m]\n",
    "\n",
    "with open(os.path.join(cu.PATH_TO_CBG_POI_DATA, 'US/visits.pkl'), 'wb') as f:\n",
    "    pickle.dump(visits, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(cu.PATH_TO_CBG_POI_DATA, 'US/index.pkl'), 'wb') as f:\n",
    "    pickle.dump({'weeks':weeks, 'pois':pois, 'cbgs':cbgs, 'poi_cbgs':poi_cbgs}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "poi_dwell.to_csv(os.path.join(cu.PATH_TO_CBG_POI_DATA, 'US/median_dwell.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CBG device counts\n",
    "results = {'census_block_group':cbgs}\n",
    "week_order = []\n",
    "for path in [path_1, path_2, path_3]:\n",
    "    path_is_pre = 'weekly_pre' in path\n",
    "    week_paths = sorted(glob.glob(os.path.join(path, '*')) if path_is_pre else glob.glob(os.path.join(path, '*/*/*/*')))\n",
    "    for week_path in week_paths: # Loop over each week in the data collection period\n",
    "        week_str = week_path.split('/')[-1][:10] if path_is_pre else '-'.join(week_path.split('/')[-4:-1])\n",
    "        if (week_str == '2020-12-09') and (path == path_3): # Duplicates in path_2 and path_3\n",
    "            continue\n",
    "        home_summary_panel = helper.load_weekly_home_panel_summary(week_str)   \n",
    "        home_summary_panel = home_summary_panel.set_index('census_block_group')\n",
    "        home_summary_panel = home_summary_panel.loc[cbgs]\n",
    "        week_start = home_summary_panel.iloc[0].date_range_start.split('T')[0]\n",
    "        print(week_str, week_start)\n",
    "        week_order.append(week_start)\n",
    "        results[week_start] = home_summary_panel['number_devices_residing'].values\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df = summary_df.set_index('census_block_group')[week_order]\n",
    "summary_df = summary_df.fillna(0).astype(int)\n",
    "fn = os.path.join(cu.PATH_TO_CBG_POI_DATA, 'cbg_device_counts.csv')\n",
    "summary_df.to_csv(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep `visits`, `index`, and `poi_attrs`  for California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cu.PATH_TO_CBG_POI_DATA, 'US/index.pkl'), 'rb') as f:\n",
    "    indices = pickle.load(f)\n",
    "    weeks = np.array(indices['weeks'])\n",
    "    cbgs = indices['cbgs']\n",
    "    pois = indices['pois']\n",
    "    poi_cbgs = indices['poi_cbgs']\n",
    "\n",
    "with open(os.path.join(cu.PATH_TO_CBG_POI_DATA, 'US/visits.pkl'), 'rb') as f:\n",
    "    visits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23212 CBGs found in CA\n",
      "614847 POIs found in CA\n"
     ]
    }
   ],
   "source": [
    "state_of_interest = 'CA'\n",
    "state_code_of_interest = 6\n",
    "mask_fn = np.vectorize(lambda cbg: helper.extract_state_code_fr_fips(cbg) == state_code_of_interest)\n",
    "\n",
    "# Filter CBGs\n",
    "mask  = mask_fn(cbgs)\n",
    "cbgs = [cbg for cbg, m in zip(cbgs, mask) if m]\n",
    "print('%d CBGs found in %s' % (len(cbgs), state_of_interest))\n",
    "state_visits = [m[mask, :] for m in visits]\n",
    "\n",
    "# Filter POIs\n",
    "mask = mask_fn(poi_cbgs)\n",
    "pois = [poi for poi, m in zip(pois, mask) if m]\n",
    "print('%d POIs found in %s' % (len(pois), state_of_interest))\n",
    "poi_cbgs = [poi_cbg for poi_cbg, m in zip(poi_cbgs, mask) if m]\n",
    "state_visits = [m[:, mask] for m in state_visits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 unique counties\n",
      "[6001 6003 6005 6007 6009 6011 6013 6015 6017 6019 6021 6023 6025 6027\n",
      " 6029 6031 6033 6035 6037 6039 6041 6043 6045 6047 6049 6051 6053 6055\n",
      " 6057 6059 6061 6063 6065 6067 6069 6071 6073 6075 6077 6079 6081 6083\n",
      " 6085 6087 6089 6091 6093 6095 6097 6099 6101 6103 6105 6107 6109 6111\n",
      " 6113 6115]\n"
     ]
    }
   ],
   "source": [
    "counties = {helper.extract_county_code_fr_fips(c) for c in cbgs}\n",
    "print('Found %d unique counties' % len(counties))\n",
    "indices = {'cbgs':np.array(cbgs),\n",
    "           'pois':np.array(pois),\n",
    "           'poi_cbgs':np.array(poi_cbgs),\n",
    "           'counties':np.array(sorted(list(counties))),\n",
    "           'weeks':np.array(indices['weeks'])}\n",
    "with open(os.path.join(cu.PATH_TO_CBG_POI_DATA, state_of_interest, 'index.pkl'), 'wb') as f:\n",
    "    pickle.dump(indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part1.csv.gz\n",
      "[########################################] | 100% Completed | 22.9s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part2.csv.gz\n",
      "[########################################] | 100% Completed | 15.8s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part3.csv.gz\n",
      "[########################################] | 100% Completed | 19.1s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part4.csv.gz\n",
      "[########################################] | 100% Completed | 17.4s\n",
      "Loading /dfs/project/safegraph-homes/all_aggregate_data/raw_safegraph_data/core_places/2020/10/core_poi-part5.csv.gz\n",
      "[########################################] | 100% Completed | 16.6s\n",
      "Loading core places info for 5333501 POIs\n"
     ]
    }
   ],
   "source": [
    "cols_to_keep_places = [\n",
    "    'safegraph_place_id',\n",
    "    'city',\n",
    "    'region',\n",
    "    'top_category',\n",
    "    'sub_category',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'open_hours'\n",
    "]\n",
    "places = helper.load_core_places_data(cols_to_keep_places)\n",
    "poi_attrs = places.loc[pois, ['sub_category', 'top_category', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing area for 36889 (6.00%) POIs\n"
     ]
    }
   ],
   "source": [
    "poi_areas = pd.read_csv(cu.PATH_TO_SAFEGRAPH_AREAS, index_col='safegraph_place_id')\n",
    "poi_areas = poi_areas.reindex(pois).area_square_feet\n",
    "missing = pd.isna(poi_areas).sum()\n",
    "print('Missing area for %d (%.2f%%) POIs' % (missing, 100. * missing / len(poi_areas)))\n",
    "poi_attrs['area_square_feet'] = poi_areas.fillna(np.nanmedian(poi_areas)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_category</th>\n",
       "      <th>top_category</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>area_square_feet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safegraph_place_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sg:00558d9ab88d4ad9998e3bb43c3f6f60</th>\n",
       "      <td>Insurance Agencies and Brokerages</td>\n",
       "      <td>Agencies, Brokerages, and Other Insurance Rela...</td>\n",
       "      <td>33.124147</td>\n",
       "      <td>-117.275411</td>\n",
       "      <td>11220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg:008eaaf6ef3a43bea9757af6a79ff239</th>\n",
       "      <td>Commercial Banking</td>\n",
       "      <td>Depository Credit Intermediation</td>\n",
       "      <td>37.736914</td>\n",
       "      <td>-122.198516</td>\n",
       "      <td>27527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg:01ba472a875d402e95738e6a93cd6e2f</th>\n",
       "      <td>Snack and Nonalcoholic Beverage Bars</td>\n",
       "      <td>Restaurants and Other Eating Places</td>\n",
       "      <td>34.019287</td>\n",
       "      <td>-118.455474</td>\n",
       "      <td>5418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg:01bd97e5e4d34ca5a3fa3521e515495a</th>\n",
       "      <td>Religious Organizations</td>\n",
       "      <td>Religious Organizations</td>\n",
       "      <td>33.811085</td>\n",
       "      <td>-118.057272</td>\n",
       "      <td>17667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg:02b54fb3e3a14f7bab06f145c30bddb6</th>\n",
       "      <td>Furniture Stores</td>\n",
       "      <td>Furniture Stores</td>\n",
       "      <td>33.891485</td>\n",
       "      <td>-118.170562</td>\n",
       "      <td>8485.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             sub_category  \\\n",
       "safegraph_place_id                                                          \n",
       "sg:00558d9ab88d4ad9998e3bb43c3f6f60     Insurance Agencies and Brokerages   \n",
       "sg:008eaaf6ef3a43bea9757af6a79ff239                    Commercial Banking   \n",
       "sg:01ba472a875d402e95738e6a93cd6e2f  Snack and Nonalcoholic Beverage Bars   \n",
       "sg:01bd97e5e4d34ca5a3fa3521e515495a               Religious Organizations   \n",
       "sg:02b54fb3e3a14f7bab06f145c30bddb6                      Furniture Stores   \n",
       "\n",
       "                                                                          top_category  \\\n",
       "safegraph_place_id                                                                       \n",
       "sg:00558d9ab88d4ad9998e3bb43c3f6f60  Agencies, Brokerages, and Other Insurance Rela...   \n",
       "sg:008eaaf6ef3a43bea9757af6a79ff239                   Depository Credit Intermediation   \n",
       "sg:01ba472a875d402e95738e6a93cd6e2f                Restaurants and Other Eating Places   \n",
       "sg:01bd97e5e4d34ca5a3fa3521e515495a                            Religious Organizations   \n",
       "sg:02b54fb3e3a14f7bab06f145c30bddb6                                   Furniture Stores   \n",
       "\n",
       "                                      latitude   longitude  area_square_feet  \n",
       "safegraph_place_id                                                            \n",
       "sg:00558d9ab88d4ad9998e3bb43c3f6f60  33.124147 -117.275411           11220.0  \n",
       "sg:008eaaf6ef3a43bea9757af6a79ff239  37.736914 -122.198516           27527.0  \n",
       "sg:01ba472a875d402e95738e6a93cd6e2f  34.019287 -118.455474            5418.0  \n",
       "sg:01bd97e5e4d34ca5a3fa3521e515495a  33.811085 -118.057272           17667.0  \n",
       "sg:02b54fb3e3a14f7bab06f145c30bddb6  33.891485 -118.170562            8485.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_attrs.to_csv(os.path.join(cu.PATH_TO_CBG_POI_DATA, state_of_interest, 'poi_attrs.csv'))\n",
    "poi_attrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep county-level tiers in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get constructed Z variable\n",
    "def get_large_Z_met(case_rate, test_pos, health_equity, vaccine_stage):  # large county\n",
    "    if vaccine_stage == 0:  # before statewide vaccine equity goal #1 was met\n",
    "        case_rate -= 7  # above is purple, below is red\n",
    "    else:  # after vaccine equity goal #1\n",
    "        case_rate -= 10\n",
    "    test_pos -= 8  # above is purple, below is red\n",
    "    health_equity -= 8\n",
    "    return np.max([case_rate, test_pos, health_equity])\n",
    "\n",
    "def get_small_Z_met(case_rate, test_pos, vaccine_stage):  # small county\n",
    "    if vaccine_stage == 0:\n",
    "        case_rate -= 7  \n",
    "    else:\n",
    "        case_rate -= 10\n",
    "    test_pos -= 8\n",
    "    return np.max([case_rate, test_pos])\n",
    "\n",
    "def get_Z_acc(test_pos, health_equity):        \n",
    "    test_pos -= 5\n",
    "    health_equity -= 5\n",
    "    return np.max([test_pos, health_equity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch1/safegraph_homes/covid_mobility_venv/covid_mobility_venv/lib/python3.7/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/dfs/scratch1/safegraph_homes/covid_mobility_venv/covid_mobility_venv/lib/python3.7/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-02 00:00:00 1\n",
      "      county  expected  actual  actual_tier\n",
      "21  Mariposa         0       1            2\n",
      "45   Sierra*         0       1            3\n",
      "2021-02-09 00:00:00 1\n",
      "       county  expected  actual  actual_tier\n",
      "21  Mariposa*         0       1            2\n",
      "2021-02-16 00:00:00 1\n",
      "      county  expected  actual  actual_tier\n",
      "21  Mariposa         0       1            2\n",
      "2021-02-23 00:00:00 1\n",
      "all matched!\n",
      "2021-03-02 00:00:00 1\n",
      "all matched!\n",
      "2021-03-13 00:00:00 5\n",
      "        county  expected  actual  actual_tier\n",
      "2     Amador**         0       1            2\n",
      "34  San Benito         0       1            2\n",
      "48      Sonoma         0       1            2\n",
      "2021-03-16 00:00:00 1\n",
      "   county  expected  actual  actual_tier\n",
      "2  Amador         0       1            2\n",
      "2021-03-23 00:00:00 1\n",
      "           county  expected  actual  actual_tier\n",
      "49  Stanislaus***         0       1            2\n",
      "2021-03-30 00:00:00 1\n",
      "        county  expected  actual  actual_tier\n",
      "49  Stanislaus         0       1            2\n"
     ]
    }
   ],
   "source": [
    "datestrings = ['012621', '020221', '020921', '021621', '022321', '030221', '031321',\n",
    "               '031621', '032321', '033021']\n",
    "all_cases = np.zeros((len(datestrings), 58))\n",
    "all_test_pos = np.zeros((len(datestrings), 58))\n",
    "all_health_eq = np.zeros((len(datestrings), 58))\n",
    "Z_met = np.zeros((len(datestrings), 58))\n",
    "Z_acc = np.zeros((len(datestrings), 58))\n",
    "Z_hat = np.zeros((len(datestrings), 58))\n",
    "Z_fin = np.zeros((len(datestrings), 58))\n",
    "T = np.zeros((len(datestrings), 58))\n",
    "population = None\n",
    "datetimes = []\n",
    "blueprint_stages = []\n",
    "\n",
    "for t, ds in enumerate(datestrings):\n",
    "    dt = datetime.datetime.strptime(ds, '%m%d%y')\n",
    "    datetimes.append(dt)\n",
    "    directory = os.path.join(cu.BASE_DIR, 'external_datasets_for_aggregate_analysis/blueprints_cdph')\n",
    "    fn = os.path.join(directory, 'Blueprint_Data_Chart_%s.xlsx' % ds)\n",
    "    df = pd.read_excel(fn, header=1)\n",
    "    df = df.iloc[:58]\n",
    "    cols = {'tier for week':None, 'previous tier assignment':None, 'final tier assignment':None}\n",
    "    for orig_col in df.columns:\n",
    "        for k in cols:\n",
    "            if k in orig_col.lower():\n",
    "                cols[k] = orig_col\n",
    "    cols['population'] = 'Population' if 'Population' in df.columns else 'Population^'\n",
    "    if ds == '031321':  # special case where we say updated, not final\n",
    "        cols['final tier assignment'] = 'Updated Tier Assignment for 03-13-21,         03-08-21 Assessment'\n",
    "    \n",
    "    population = df[cols['population']].values\n",
    "    cases = df['Case Rate Used for Tier Adjusted Using Linear Adjustment (7-day avg, 7-day lag)'].values\n",
    "    all_cases[t] = cases\n",
    "    tests = df['Test Positivity excl prisons (7-day, 7-day lag)'].values\n",
    "    all_test_pos[t] = tests\n",
    "    health_eq = df['Health Equity Quartile Test Positivity Excl Prison Cases (7 day, 7 day lag)'].values\n",
    "    all_health_eq[t, population >= cu.LARGE_COUNTY_CUTOFF] = health_eq[population >= cu.LARGE_COUNTY_CUTOFF]\n",
    "    vaccine_stage = int(dt >= datetime.datetime(2021, 3, 12))  # when statewide vaccine equity goal #1 was met\n",
    "    blueprint_stages.append(vaccine_stage)\n",
    "    for i in range(58):\n",
    "        pop = population[i]\n",
    "        if pop < cu.LARGE_COUNTY_CUTOFF:  # small county rules\n",
    "            Z_met[t, i] = get_small_Z_met(cases[i], tests[i], vaccine_stage)\n",
    "        else:\n",
    "            Z_met[t, i] = get_large_Z_met(cases[i], tests[i], health_eq[i], vaccine_stage)\n",
    "            Z_acc[t, i] = get_Z_acc(tests[i], health_eq[i])\n",
    "    \n",
    "    if t > 0:\n",
    "        print(dt, dt.weekday())  # all Tuesdays\n",
    "        Z_met_combined = np.maximum(Z_met[t-1], Z_met[t])\n",
    "        Z_acc_combined = np.maximum(Z_acc[t-1], Z_acc[t])\n",
    "        # large counties could meet metrics OR be accelerated\n",
    "        Z_met_or_acc = np.minimum(Z_met_combined, Z_acc_combined)\n",
    "        Z_hat[t, population >= cu.LARGE_COUNTY_CUTOFF] = Z_met_or_acc[population >= cu.LARGE_COUNTY_CUTOFF]\n",
    "        # small counties have no accelerated option\n",
    "        Z_hat[t, population < cu.LARGE_COUNTY_CUTOFF] = Z_met_combined[population < cu.LARGE_COUNTY_CUTOFF]\n",
    "        Z_fin[t] = np.min(Z_hat[1:t+1], axis=0)\n",
    "        \n",
    "        expected_assignment = (Z_fin[t] < 0).astype(int)\n",
    "        actual_assignment = df[cols['final tier assignment']].values.astype(int)\n",
    "        T[t] = actual_assignment\n",
    "        actual_assignment_bin = (actual_assignment > 1).astype(int)\n",
    "        diff = expected_assignment - actual_assignment_bin\n",
    "        total = np.sum(np.abs(diff))\n",
    "        if total > 0:\n",
    "            new_df = pd.DataFrame({'county':df.County, 'expected':expected_assignment, 'actual':actual_assignment_bin,\n",
    "                                   'actual_tier':actual_assignment})\n",
    "            print(new_df[new_df.expected != new_df.actual])\n",
    "        else:\n",
    "            print('all matched!')\n",
    "    \n",
    "    if ds == '030221':  # need to rewrite case-related metrics because of change in thresholds\n",
    "        for i in range(58):\n",
    "            pop = population[i]\n",
    "            if pop < cu.LARGE_COUNTY_CUTOFF:\n",
    "                Z_met[t, i] = get_small_Z_met(cases[i], tests[i], vaccine_stage=1)\n",
    "            else:\n",
    "                Z_met[t, i] = get_large_Z_met(cases[i], tests[i], health_eq[i], vaccine_stage=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (58,)\n",
      "<class 'numpy.ndarray'> (58,)\n",
      "<class 'numpy.ndarray'> (9,)\n",
      "<class 'numpy.ndarray'> (9,)\n",
      "<class 'numpy.ndarray'> (9, 58)\n",
      "<class 'numpy.ndarray'> (9, 58)\n"
     ]
    }
   ],
   "source": [
    "bundle = (fips, population, np.array(datetimes[1:]), np.array(blueprint_stages[1:]), T[1:], Z_fin[1:])\n",
    "for x in bundle:\n",
    "    print(type(x), x.shape)\n",
    "fn = os.path.join(cu.PATH_TO_CBG_POI_DATA, 'CA/county_dynamic_attrs_2021_t1t2.pkl')\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(bundle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-02 00:00:00 7\n",
      "2021-02-09 00:00:00 7\n",
      "2021-02-16 00:00:00 7\n",
      "2021-02-23 00:00:00 7\n",
      "2021-03-02 00:00:00 7\n",
      "2021-03-13 00:00:00 10\n",
      "2021-03-16 00:00:00 10\n",
      "2021-03-23 00:00:00 10\n",
      "2021-03-30 00:00:00 10\n"
     ]
    }
   ],
   "source": [
    "# check triggering patterns\n",
    "Z_met_counts = []\n",
    "Z_acc_counts = []\n",
    "Z_hat_counts = []\n",
    "Z_fin_counts = []\n",
    "for t in range(1, len(datestrings)):\n",
    "    dt = datetime.datetime.strptime(datestrings[t], '%m%d%y')\n",
    "    if dt >= datetime.datetime(2021, 3, 12):\n",
    "        case_cutoff = 10\n",
    "    else:\n",
    "        case_cutoff = 7\n",
    "    print(dt, case_cutoff)\n",
    "    for i in range(58):\n",
    "        expected_assignment = int(Z_fin[t,i] < 0)\n",
    "        actual_assignment_bin = int(T[t,i] > 1)\n",
    "        if population[i] >= cu.LARGE_COUNTY_CUTOFF and (expected_assignment == actual_assignment_bin) and (T[t, i] in [1, 2]):\n",
    "            case_rate_w = all_cases[t,i] - case_cutoff\n",
    "            test_pos_w = all_test_pos[t,i] - 8\n",
    "            health_eq_w = all_health_eq[t,i] - 8\n",
    "            case_rate_w_prev = all_cases[t-1,i] - case_cutoff\n",
    "            test_pos_w_prev = all_test_pos[t-1,i] - 8\n",
    "            health_eq_w_prev = all_health_eq[t-1,i] - 8\n",
    "            inputs = [case_rate_w, test_pos_w, health_eq_w,\n",
    "                      case_rate_w_prev, test_pos_w_prev, health_eq_w_prev]\n",
    "            Z_met_counts.append(np.argmax(inputs))\n",
    "            Z_met_it = np.max(inputs)\n",
    "            \n",
    "            test_pos_w = all_test_pos[t,i] - 5\n",
    "            health_eq_w = all_health_eq[t,i] - 5\n",
    "            test_pos_w_prev = all_test_pos[t-1,i] - 5\n",
    "            health_eq_w_prev = all_health_eq[t-1,i] - 5\n",
    "            inputs = [test_pos_w, health_eq_w, test_pos_w_prev, health_eq_w_prev]\n",
    "            Z_acc_counts.append(np.argmax(inputs))\n",
    "            Z_acc_it = np.max(inputs)\n",
    "            \n",
    "            inputs = [Z_met_it, Z_acc_it]\n",
    "            Z_hat_counts.append(np.argmin(inputs))\n",
    "            Z_hat_it = np.min(inputs)\n",
    "            assert Z_hat_it == Z_hat[t,i]\n",
    "            Z_fin_counts.append(int(np.argmin(Z_hat[1:t+1,i]) == (t-1)))  # 0 if prev is min, 1 if curr is min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max over metrics Counter({3: 244, 5: 25, 0: 15, 2: 12})\n",
      "Max over accelerated Counter({3: 231, 1: 42, 2: 19, 0: 4})\n",
      "Min over two rules Counter({1: 188, 0: 108})\n",
      "Min over prev and curr Counter({1: 260, 0: 36})\n"
     ]
    }
   ],
   "source": [
    "print('Max over metrics', Counter(Z_met_counts))\n",
    "print('Max over accelerated', Counter(Z_acc_counts))\n",
    "print('Min over two rules', Counter(Z_hat_counts))\n",
    "print('Min over prev and curr', Counter(Z_fin_counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_mobility_venv",
   "language": "python",
   "name": "covid_mobility_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
